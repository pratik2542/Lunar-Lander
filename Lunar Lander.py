# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wyzY8PAVup_Nv0JVfCt8mP0s0Uilxhpz
"""

!pip install --upgrade tensorflow

!pip install Box2D

!sudo apt-get install -y xvfb ffmpeg
!pip install 'gym==0.10.11'
!pip install 'imageio==2.4.0'
!pip install PILLOW
!pip install 'pyglet==1.3.2'
!pip install pyvirtualdisplay
!pip install --upgrade tensorflow-probability
!pip install tf-agents
!pip install tf-agents[reverb]

import tensorflow as tf
import base64
import imageio
import IPython
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import PIL.Image
import pyvirtualdisplay
import reverb

from tf_agents.agents.dqn import dqn_agent
from tf_agents.drivers import py_driver
from tf_agents.environments import suite_gym
from tf_agents.environments import tf_py_environment
from tf_agents.eval import metric_utils
from tf_agents.metrics import tf_metrics
from tf_agents.networks import sequential
from tf_agents.policies import py_tf_eager_policy
from tf_agents.policies import random_tf_policy
from tf_agents.replay_buffers import reverb_replay_buffer
from tf_agents.replay_buffers import reverb_utils
from tf_agents.trajectories import trajectory
from tf_agents.specs import tensor_spec
from tf_agents.utils import common
from tf_agents.agents.categorical_dqn import categorical_dqn_agent
from tf_agents.networks import categorical_q_network

from tf_agents.specs import tensor_spec
from tf_agents.trajectories import time_step as ts

display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()

from tf_agents.environments import suite_gym

env = suite_gym.load("LunarLander-v2")
env.reset()
PIL.Image.fromarray(env.render())

from tf_agents.environments.tf_py_environment import TFPyEnvironment
tf_env = TFPyEnvironment(env)

print('Observation Spec: ')
print(env.time_step_spec().observation)

print('Action Spec: ')
print(env.action_spec())

from tf_agents.networks.q_network import QNetwork

fc_layer_params = [512,256] # number of neurons in hidden layers

q_network = QNetwork(
    tf_env.observation_spec(),
    tf_env.action_spec(),
    fc_layer_params=fc_layer_params
)

from tf_agents.agents.dqn.dqn_agent import DqnAgent

from tensorflow import keras

train_step = tf.Variable(0)
update_period = 5

optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3)

epsilon_fn = keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=0.07, decay_steps=250000 // update_period,end_learning_rate=0.001)

agent = DqnAgent(
    tf_env.time_step_spec(),
    tf_env.action_spec(),
    q_network=q_network,
    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3),
    td_errors_loss_fn= common.element_wise_squared_loss,
    gamma = 0.95,
    train_step_counter=train_step,
    epsilon_greedy=lambda: epsilon_fn(train_step)
)

agent.initialize()

from tf_agents.replay_buffers import tf_uniform_replay_buffer

replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(data_spec=agent.collect_data_spec,
                                                               batch_size=tf_env.batch_size,
                                                               max_length = 2000)

replay_buffer_observer = replay_buffer.add_batch

from tf_agents.metrics import tf_metrics

train_metrics = [tf_metrics.NumberOfEpisodes(),
              tf_metrics.EnvironmentSteps(),
              tf_metrics.AverageReturnMetric(),
              tf_metrics.AverageEpisodeLengthMetric()]

from tf_agents.eval.metric_utils import log_metrics
import logging

logging.getLogger().setLevel(logging.INFO)

log_metrics(train_metrics)

from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver

collect_driver = DynamicStepDriver(tf_env, 
                                   agent.collect_policy, 
                                   observers=[replay_buffer_observer] + train_metrics,
                                   num_steps=update_period)

from tf_agents.policies.random_tf_policy import RandomTFPolicy

class ShowProgress:
  def __init__(self, total):
    self.counter = 0
    self.total = total
  def __call__(self, trajectory):
    if not trajectory.is_boundary():
      self.counter += 1
    if self.counter % 100 == 0:
      print("\r{}/{}".format(self.counter, self.total), end="")

initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),tf_env.action_spec())

init_driver = DynamicStepDriver(tf_env,initial_collect_policy,observers=[replay_buffer.add_batch, ShowProgress(20000)],
                                num_steps=20000)

final_time_step,final_policy_state = init_driver.run()

trajectories, buffer_info = replay_buffer.get_next(sample_batch_size =2, num_steps=3)

print(trajectories._fields)
print(trajectories.observation.shape)
trajectories.step_type.numpy ()

from tf_agents.trajectories.trajectory import to_transition

time_steps,action_steps,next_time_steps = to_transition(trajectories)

dataset = replay_buffer.as_dataset(sample_batch_size = 128,
                                   num_steps = 2,
                                   num_parallel_calls=3).prefetch(3)

def agent_train(n_iterations):
  time_step = None
  policy_state = agent. collect_policy.get_initial_state (tf_env.batch_size)
  iterator = iter (dataset)
  for iteration in range (n_iterations):
    time_step, policy_state = collect_driver.run(time_step, policy_state)
    trajectories, buffer_info = next (iterator)
    train_loss = agent.train(trajectories)
    print("\r{} loss:{:.5f}".format(
        iteration, train_loss.loss.numpy()
    ),end="")

    if iteration & 1000 == 0:
      log_metrics(train_metrics)

agent_train(1000)

import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from mpl_toolkits.mplot3d import axes3d
from matplotlib import style

mpl.rc('animation', html='jshtml')

def update_scene(num, frames, patch):
  patch.set_data(frames[num])
  return patch,

def plot_animation(frames, repeat=False,interval=15):
  fig = plt.figure()
  patch = plt.imshow(frames[0])
  plt.axis('off')

  anim = animation.FuncAnimation(fig, update_scene, fargs=(frames, patch),
                                 frames = len(frames),repeat=repeat,interval=interval)
  plt.close()
  return anim
frames = []

def save_frames(trajectory):
  global frames
  frames.append(tf_env.pyenv.envs[0].render(mode = "rgb_array"))

watch_driver = DynamicStepDriver(
    tf_env,
    agent.policy,
    observers=[save_frames, ShowProgress(1000)],
    num_steps=1000
)

final_time_step, final_policy_state = watch_driver.run()

plot_animation(frames)

from IPython.display import HTML
HTML(plot_animation(frames).to_html5_video())

